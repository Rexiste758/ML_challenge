# #### Importamos las librerias necesarias

import os
import mysql.connector
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.stats import zscore
from pyod.models.mad import MAD
from scipy.stats import pearsonr
from sklearn.model_selection import train_test_split
from scipy import stats
from scipy.stats import zscore
import joblib
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression


# Obtener las variables de entorno
db_host = os.getenv('DB_HOST', 'localhost')
db_user = os.getenv('DB_USER', 'myuser')
db_password = os.getenv('DB_PASSWORD', 'mypassword')
db_name = os.getenv('DB_NAME', 'mydatabase')



# #### Realizamos la conexi칩n a la base de datos de MySQL, previamente creada (usando el archivo csv compartido). Al mismo tiempo que, creamos una excepci칩n en caso de que se llegu칠 a generar un mensaje de error.

# Conectar a la base de datos MySQL

connection = mysql.connector.connect(
    host=db_host,
    user=db_user,
    password=db_password,
    database=db_name
)

print("Conectado a la base de datos MySQL")




# #### Una vez realizada la conexi칩n, obtenemos todo el contenido de la tabla mediente una sentencia de SQL y almacenamos el contenido en la variable df.


df = pd.read_sql_query("select * from training_dataset", connection)


# #### Como parte del primer acercamiento hacia la data, buscamos identificar valores nulos dentro de la tabla que pudier치n llegar a afectar el rendimiento del modelo. Especialmente en los siguientes aspectos:
# 
# Sesgos:
# Si los valores nulos no se manejan adecuadamente, podr칤an introducir sesgos en el modelo.
# 
# P칠rdida de precisi칩n:
# Las columnas o filas con muchos valores nulos pueden aportar poca informaci칩n al modelo, reduciendo la capacidad predictiva.


null_counts = df.isnull().sum()
print(null_counts)


# En este caso, vemos que no se cuentan con valores nulos por lo que no es necesario realizar alguna imputacion de datos.

# #### De igual forma, obtenemos el tipo de dato por columna. Esto para que ver que en la lectura se haya identificado el tipo de dato correcto. 


print(df.dtypes)


# #### Como segundo punto del analisis que buscamos realizar hacia la data. Usamos la funci칩n describe() esto, para identificar la distribuci칩n de la data y las magnitudes de cada columna.


print(df[['age', 'bmi', 'children', 'charges']].describe())


# Logramos identificar que:
# - La columna "charges" tiene una diferencia en maginitudes en relaci칩n con las dem치s variables, esto puede ocasionar un mal    rendimiento y afectar la precisi칩n del modelo. Por otro lado, despu칠s de observar los cuantiles vemos que el 75% de las personas tienen un costo m칠dico menor o igual a 16,639.91 y un precio promedio de 13, 270.422265 y valor m치ximo de 63,770.428010, esto nos dice que la distribuci칩n se encuentra sesgada hacia la izquierda. 
# 
# - La columna "bmi" presenta una distribuci칩n normal ya que la media de valores se encuentran en torno al 30.663397, un valor minimo de 15.960000 y un valor m치ximo de 53.130000
# 
# - Con la columna "children" vemos que, el 75% de las personas tienen 2 hijos o menos.

# Especificamos las columnas que quiero incluir en el pairplot
columnas_interes = ["age", "charges", "bmi"]

# Creamos el pairplot para las columnas seleccionadas
sns.pairplot(data=df, vars=columnas_interes)

# Mostramos el gr치fico

plt.show()


# De manera visual, podemos ver que:
# - Entre las variables "age" y "charges" existe una relaci칩n positiva: a medida que la edad ("age") aumenta, los cargos ("charges") tienden a aumentar tambi칠n.
# - Entre las variables "bmi" y "charges" la relaci칩n parece no lineal: A medida que el 칤ndice de masa corporal (bmi) aumenta, tambi칠n lo hacen los cargos (charges), pero la dispersi칩n es alta.Tambi칠n podemos ver que, hay casos particulares de personas con charges muy elevados, lo que sugiere que podr칤a haber otros factores, como condiciones de salud o el h치bito de fumar, que influyen en esta relaci칩n.
# - La distribuci칩n de "charges" sugiere la posibilidad de realizar transformaciones como el logaritmo para estabilizar la varianza.


df['log_charges'] = np.log(df['charges'] )


# Realizamos la transformaci칩n de la variable "charges" aplicando la funci칩n de logartimo y almacenando los nuevos valores en una  columna llamada "log_charges".

# #### Creamos BOXPLOT para las columnas "age", "bmi" y "charges"  y de esa forma identificar de manera visual tanto la distribuci칩n de los datos como los valores "outliers".

# Creamos una figura con subgr치ficos
fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=False)

# Variables a graficar
variables = ['age', 'bmi', 'charges']

# Graficamos cada variable en un subgr치fico
for ax, var in zip(axes, variables):
    g = sns.boxplot(data=df, x=var, ax=ax)
    g.set_title(f'Box Plot de {var.capitalize()}')
    g.set_xlabel(f'{var.capitalize()}')

# Ajustamos el espaciado
plt.tight_layout()
plt.show()


# Logramos identificar que: 
# 
# ##### Box Plot de Age (Edad):
# 
# La distribuci칩n parece ser bastante sim칠trica.
# No se observan outliers (valores at칤picos) significativos, ya que no hay puntos dispersos fuera de los bigotes.
# La mayor칤a de los datos de edad se concentran entre aproximadamente 20 y 60 a침os.
# 
# ##### Box Plot de Bmi (칈ndice de Masa Corporal):
# 
# Aqu칤 s칤 se observan outliers a la derecha del diagrama, lo que sugiere que existen valores altos de Bmi que son at칤picos en comparaci칩n con el resto de los datos.
# El rango intercuart칤lico (IQR) est치 entre aproximadamente 25 y 35, donde se concentra la mayor칤a de los datos.
# 
# ##### Box Plot de Charges (Costos o Cargos):
# 
# En este caso, hay una gran cantidad de outliers en la parte superior del diagrama, lo que indica que existen valores extremadamente altos de Charges en comparaci칩n con el resto de la muestra.
# Esto sugiere una distribuci칩n sesgada a la derecha, ya que la mayor칤a de los datos se concentran en valores bajos y hay una cola larga hacia valores m치s altos.

# #### Mostramos los valores 칰nicos y los conteos para cada columna categorica, esto nos ayudar치 a:
# 
# -Detectar datos inconsistentes o errores.
# 
# -Entender la distribuci칩n de las categor칤as.
# 

columns_to_check = ['sex', 'children', 'smoker', 'region']

# Mostramos valores 칰nicos y sus conteos para cada columna seleccionada
for column in columns_to_check:
    print(f"Valores 칰nicos y conteos en la columna '{column}':")
    print(df[column].value_counts())
    print()


# Vemos que, no existen datos inconsistes o errores dentro de las variables categoricas. Vale la pena mencionar que, los "outliers" como tal no se aplican a las variables categ칩ricas en el mismo sentido que a las variables num칠ricas ya que en variables categoricas unicamente se logran identificar frecuencias.

# ### Extracci칩n de valores "outliers"

# #### Z-Score
# 
# Un Z-Score, tambi칠n conocido como puntuaci칩n Z, es una medida estad칤stica que indica cu치ntas desviaciones est치ndar un punto de datos espec칤fico est치 por encima o por debajo de la media del conjunto de datos.
# 
# En este sentido, podemos calcular la puntuaci칩n Z de cada dato usando la funci칩n Z-Score de scipy y compararla con un umbral para determinar qu칠 valores son considerados at칤picos. Por lo general se establece un umbral de 3, por lo que aquellos puntos de datos cuya puntuaci칩n Z absoluta sea superior a 3 son outliers.
# 
# #### Recordando que: Z-Score s칩lo es apropiada para distribuciones normales

# Calculamos el z-score para la columna 'bmi'

z_scores = zscore(df['bmi'])
abs_z_scores = np.abs(z_scores)

# Creamos una nueva columna llamada 'outliers' que nos ayudar치 a marcar si una fila es un outlier.
df['outliers'] = abs_z_scores > 3


print(f'N칰mero de outliers totales para la columna bmi: {df["outliers"].sum()}')


# #### Z-Score modificado

# Cuando los datos son asim칠tricos o no se distribuyen de forma normal podemos utilizar el Z-score modificado, tambi칠n conocido como MAD-Z-Score. Este, a diferencia del z-score, utiliza la mediana y la desviaci칩n absoluta mediana (MAD en ingl칠s) en lugar de la media y la desviaci칩n est치ndar con el fin de evitar el efecto de los outliers sobre estas dos 칰ltimas medidas.
# Se recomienda que los valores con puntuaciones z modificadas inferiores a -3,5 o superiores a 3,5 se etiqueten como posibles valores at칤picos.


# Configuramos MAD con un threshold de 3.5
mad = MAD(threshold=3.5)

# Analizamos los outliers en la columna 'charges'
charges_reshaped = df['charges'].values.reshape(-1, 1)
charges_labels = mad.fit(charges_reshaped).labels_

# Analizamos los outliers en la columna 'age'
age_reshaped = df['age'].values.reshape(-1, 1)
age_labels = mad.fit(age_reshaped).labels_

# Actualizamos la columna 'outliers' en el DataFrame ahora usando las columnas "age" y "charges"
df['outliers'] = np.where(
    (df['outliers'] == True) | (charges_labels == 1) | (age_labels == 1),
    True,
    False
)

# Imprimimos el n칰mero de outliers y el DataFrame con la columna actualizada
print(f'N칰mero de outliers totales (incluyendo los valores de la columna "bmi"): {df["outliers"].sum()}')


# En este caso, imprimimos la suma de los valores de la columna "outliers". Esta columna nos ayuda a identificar los valores que se clasifican como "outliers" para todas las columnas n칰mericas ("bmi", "age" y "charges").

# Sobreescribimos el DataFrame original eliminando las filas con outliers
df = df[df['outliers'] == False].copy()

# Imprimimos el n칰mero de filas restantes. despues de eliminar los "outliers"
print(f'N칰mero de filas restantes despu칠s de eliminar los "outliers": {len(df)}')

# Eliminamos la columna 'outliers'
df.drop(columns=['outliers'], inplace=True)


# Despu칠s de, identificar los valores "outliers" dentro del dataframe para las variables n칰mericas eliminamos los registros para:
# - Eliminar sesgos en los resultados del modelo.
# - Evitar reducir la precisi칩n y generalizaci칩n del modelo porque los algoritmos intentan ajustarse a estos puntos anormales, en lugar de centrarse en el patr칩n general de los datos.
# - Evitar la interpretaci칩n erronea ya que los outliers pueden dificultar la interpretaci칩n de patrones y relaciones entre las variables.

# #### Codificaci칩n de variables categ칩ricas

# Las variables categ칩ricas representan atributos discretos que no se pueden interpretar como valores num칠ricos directos. Al codificar estas variables de manera adecuada, podemos transformarlas en formatos num칠ricos comprensibles para los algoritmos de modelado.

# Por lo que, usaremos la codifcaci칩n ordinal para asignar un valor numerico a cada valor que puede tomar una columna categorica.

# Definimos el orden de las categor칤as para las columnas
encoding_orders = {
    'sex': ['male', 'female'],
    'smoker': ['no', 'yes'],
    'region': ['southeast','southwest','northwest','northeast']
}

# Aplicamos la codificaci칩n ordinal para cada columna
for column, order in encoding_orders.items():
    df[column] = df[column].map({cat: idx for idx, cat in enumerate(order)})

# Imprimimos el DataFrame codificado
print(df)


# Despues de, identificar las frecuencias para cada columna categorica es facil definir los valores que puede tomar cada columna. 

# #### Matriz de correlaci칩n

# Creamos una matriz de correalaci칩n para entender las relaciones entre las variables en el conjunto de datos. Adem치s de, identificar variables relevantes y guiar en la mejor elecci칩n del modelo (lineal o no lineal), mejorando el rendimiento y la interpretabilidad del modelo.

# In[99]:


correlation_matrix = df.select_dtypes(include=['number']).corr()

# Creamos un mapa de calor para visualizar la matriz de correlaci칩n
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", center=0)
plt.title('Matriz de correlaci칩n de variables num칠ricas')
plt.show()


# ##### -Log_Charges y Smoker:
# 
# Existe una correlaci칩n moderada positiva de 0.49 entre smoker y log_charges. Esto indica que las personas que fuman tienden a tener costos (charges) m치s altos. Adem치s de que al aplicar el logaritmo a charges, notamos una reducci칩n en la correlaci칩n ya que estamos comprimendo la escala de valores de charges y reduciendo la influencia de los valores extremos.
# Como vemos, esto disminuye la magnitud de la correlaci칩n, ya que los valores altos de charges (que estaban fuertemente asociados con smoker) tienen menos peso despu칠s de la transformaci칩n.
# 
# 
# ##### -Log_Charges y BMI:
# 
# El valor -0.024 est치 muy cercano a 0, lo que indica que no hay una relaci칩n lineal apreciable entre las dos variables. Aplicar una transformaci칩n logar칤tmica en bmi no tendr칤a un efecto significativo porque el problema no es la asimetr칤a de la distribuci칩n, sino que bmi no est치 relacionado con log_charges ya sea de manera lineal o no lineal.
# 
# #####  -Log_Charges y Age:
# 
# La correlaci칩n es positiva moderada-alta (0.61), lo que indica que las personas de mayor edad tienden a tener mayores costos. Adem치s de que, la correlaci칩n aumenta significativamente a 0.61 en comparaci칩n con charges (0.4), indicando que la transformaci칩n logar칤tmica ayuda a capturar mejor la relaci칩n con la edad.
# 
# ##### -Relaci칩n con children, sex y region:
# 
# Las variables children, sex y region muestran correlaciones muy bajas con charges y log_charges, lo que sugiere que no tienen un impacto significativo en estas variables.
# bmi y smoker:
# 
# ##### Resumen general:
# 
# Edad y fumador son las variables que m치s influyen en los costos (charges), con age siendo el m치s relevante.
# La transformaci칩n logar칤tmica de charges mejora su relaci칩n con variables como age.
# Variables como children, sex y region no muestran correlaciones importantes con charges.

# In[100]:


columnas = ['age', 'bmi', 'charges','log_charges']

sns.pairplot(
    df[columnas], kind='reg', diag_kind='kde', plot_kws={'line_kws': {'color': 'red'}}
)
plt.tight_layout()
plt.show()


# 游댮 La l칤nea roja representa una regresi칩n ajustada para identificar tendencias entre las variables.
# 
# ##### Variables involucradas:
# 
# - age (edad)
# - bmi (칤ndice de masa corporal)
# - charges (cargos o costos)
# - log_charges (logaritmo natural de los cargos)
# 
# ##### Observaciones generales:
# 
# - Relaci칩n entre charges y age:
# Existe una relaci칩n positiva: a medida que aumenta la edad, los cargos (charges) tienden a incrementarse.
# 
# - Relaci칩n entre BMI y charges:
# No hay una relaci칩n fuerte aparente entre el 칤ndice de masa corporal (bmi) y los cargos (charges). La nube de puntos es dispersa y la l칤nea roja es relativamente plana. Lo cual indica que BMI tiene un impacto limitado en los costos.
# 
# ##### Distribuciones:
# 
# - Age presenta dos picos, lo que sugiere una distribuci칩n bimodal.
# - Charges est치 sesgada a la derecha, lo que indica que la mayor칤a de los cargos son bajos, pero hay valores muy grandes (outliers).
# - log_charges parece m치s sim칠trica, lo cual confirma que la transformaci칩n logar칤tmica logra manejar sesgos en los datos.
# 
# ##### Conclusi칩n:
# Este an치lisis sugiere que la edad (age) tiene una fuerte relaci칩n positiva con los costos (charges), mientras que otras variables como BMI no muestran una relaci칩n clara. La transformaci칩n logar칤tmica de charges es 칰til para reducir el sesgo en la distribuci칩n.

# ### Regresi칩n Polinomial 

# #### 1. Relaci칩n no lineal entre algunas variables y charges
# En el an치lisis que realizamos:
# Observamos que age ten칤a una correlaci칩n moderada (0.4) con charges, pero al aplicar el logaritmo, mejor칩 a 0.61.
# La regresi칩n polinomial puede capturar relaciones no lineales m치s complejas, en este caso, los costos m칠dicos (charges) aumentan con la edad, pero no de manera lineal (relaci칩n cuadr치tica).
# Justificaci칩n: La regresi칩n polinomial permite modelar estas relaciones curvil칤neas que no pueden ser capturadas por un modelo lineal simple.
# 
# #### 2. Evidencia de no linealidad en los gr치ficos
# En los gr치ficos de dispersi칩n que vimos arriba:
# La relaci칩n entre age y charges parec칤a mostrar curvatura (un patr칩n no completamente lineal).
# La variable bmi tambi칠n podr칤a tener patrones m치s complejos, especialmente en combinaci칩n con otras variables como smoker.
# 
# Justificaci칩n: Esto nos dice que, un modelo lineal simple podr칤a ser insuficiente para capturar la verdadera relaci칩n entre las variables y charges.La inclusi칩n de t칠rminos polin칩micos (como age^2 o bmi^2) puede ajustar mejor los patrones observados en los datos.
# 
# #### 3. Interacci칩n entre variables categ칩ricas y continuas
# La variable smoker tiene una fuerte relaci칩n con charges (correlaci칩n de 0.49).
# Esto indica que ser fumador o no fumador influye significativamente en los costos.
# Sin embargo, el efecto de bmi o age podr칤a depender de si la persona es fumadora o no (interacciones no lineales).
# Por ejemplo, el impacto del BMI en los costos podr칤a ser m치s pronunciado para fumadores y menos relevante para no fumadores.
# 
# Justificaci칩n: Las interacciones no lineales y los efectos combinados de las variables categ칩ricas (smoker, sex, region) con las continuas (age, bmi) pueden ser capturados mejor con t칠rminos polin칩micos.
# 
# #### 4. Mejora en las m칠tricas del modelo
# Comparar un modelo lineal simple con un modelo polinomial:
# Calcular m칠tricas como R^2 ajustado, RMSE (Error cuadr치tico medio) y MAE.
# Si el modelo polinomial reduce significativamente el error sin caer en sobreajuste, se justifica su uso.
# Realizar validaci칩n cruzada para asegurarse de que el modelo generaliza bien en datos nuevos.
# 
# Justificaci칩n: Si el modelo polinomial mejora las m칠tricas de ajuste y precisi칩n en comparaci칩n con el modelo lineal, esto respalda su uso.
# 
# #### 5. Interpretaci칩n de las variables
# La regresi칩n polinomial permite entender c칩mo las variables no lineales impactan en charges:
# Por ejemplo, un t칠rmino age^2 podr칤a indicar que los costos aumentan m치s r치pidamente con la edad a partir de cierto punto.
# Un t칠rmino bmi^2 podr칤a mostrar que los costos solo se elevan significativamente cuando el BMI supera cierto umbral.
# 
# Justificaci칩n: La interpretaci칩n de los t칠rminos polin칩micos ayuda a entender mejor el comportamiento no lineal de los datos y su efecto en charges.
# 
# ##### Resumen: Argumento final
# El uso de un modelo de regresi칩n polinomial para predecir charges se justifica porque:
# 
# La relaci칩n entre age y charges sugiere una posible curvatura (no linealidad).
# El impacto del BMI podr칤a depender de umbrales cr칤ticos y combinaciones con otras variables, como smoker.
# La validaci칩n cruzada y la comparaci칩n de m칠tricas de error podr칤an demostrar que el modelo polinomial ajusta mejor los datos.
# Se pueden capturar interacciones no lineales entre variables categ칩ricas y continuas. 游
# 

X,y=df[["smoker","children","age",'region',"bmi","sex"]],df["log_charges"]
poly=PolynomialFeatures(degree=2,include_bias=False)
poly_features=poly.fit_transform(X)
X_train,X_test,y_train,y_test=train_test_split(poly_features,y,test_size=0.3,random_state=43)


# ##### 1.- Selecci칩n de variables:
# X, y = df[["smoker", "children", "age", 'region', "bmi", "sex"]], df["log_charges"]
# 
# X contiene las caracter칤sticas predictoras seleccionadas (variables independientes) del DataFrame df, mientras que y es la variable objetivo log_charges (variable dependiente).
# 
# ##### 2.-Generaci칩n de caracter칤sticas polin칩micas:
# 
# poly = PolynomialFeatures(degree=2, include_bias=False)
# poly_features = poly.fit_transform(X)
# 
# - Se utiliza PolynomialFeatures de sklearn para crear caracter칤sticas polin칩micas de grado 2.
# Esto significa que se incluir치n t칠rminos polin칩micos de todas las variables en X, pero sin incluir el t칠rmino de sesgo (constante), porque include_bias=False.
# 
# ##### 3.- Divisi칩n del conjunto de datos:
# 
# X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=43)
# 
# 
# Se utiliza train_test_split para dividir los datos en entrenamiento (70%) y prueba (30%).
# El par치metro random_state=43 asegura reproducibilidad (semilla), es decir, la divisi칩n ser치 la misma cada vez que se ejecute el c칩digo.
# 

poly_reg_model=LinearRegression()
poly_reg_model.fit(X_train,y_train)
poly_y_predict=poly_reg_model.predict(X_test)
poly_rmse=np.sqrt(mean_squared_error(y_test,poly_y_predict))
print(f"poly rmse= {poly_rmse}")


# ##### 1. Creaci칩n del modelo de regresi칩n lineal:
# poly_reg_model = LinearRegression()
# 
# Se instancia un modelo de regresi칩n lineal LinearRegression() de sklearn. Este modelo ajustar치 una l칤nea recta (o hiperplano) a los datos, aunque en este caso las caracter칤sticas polin칩micas permitir치n capturar relaciones no lineales.
# 
# ##### 2. Entrenamiento del modelo:
# poly_reg_model.fit(X_train, y_train)
# 
# El modelo se entrena utilizando los datos de entrenamiento X_train (caracter칤sticas polin칩micas) y y_train (valores reales de log_charges). La funci칩n fit() ajusta los coeficientes de la regresi칩n lineal minimizando el error cuadr치tico.
# 
# ##### 3. Predicci칩n en el conjunto de prueba:
# poly_y_predict = poly_reg_model.predict(X_test)
# 
# Una vez entrenado, el modelo predice los valores de log_charges para las caracter칤sticas del conjunto de prueba X_test.
# poly_y_predict almacena las predicciones generadas.
# 
# ##### 4. C치lculo del error (RMSE):
# poly_rmse = np.sqrt(mean_squared_error(y_test, poly_y_predict))
# 
# Se calcula el error cuadr치tico medio (MSE) entre los valores reales y_test y las predicciones poly_y_predict usando mean_squared_error.
# El RMSE (Root Mean Squared Error) se obtiene tomando la ra칤z cuadrada del MSE. Este m칠trica mide la desviaci칩n promedio entre los valores reales y las predicciones.

r2 = r2_score(y_test,poly_y_predict)
print(f"R-squared: {r2}")


# La funci칩n r2_score de sklearn calcula el coeficiente de determinaci칩n R_2, que eval칰a la calidad del modelo ajustado comparando las predicciones con los valores reales. El valor R-squared: 0.8174276353945875 indica que el modelo explica aproximadamente el 81.7% de la variabilidad de la variable objetivo (log_charges) a partir de las caracter칤sticas polin칩micas.
# Mientras m치s cerca est칠 el R_2 de 1, mejor ser치 el ajuste del modelo. Un valor de 0.817 sugiere un buen desempe침o, siempre recordando no sobreajustar el modelo.

# ##### Cross-Validation
# 
# cross_val_score realiza la validaci칩n cruzada con 5 particiones (cv=5):
# Divide los datos en 5 subconjuntos (folds).
# El modelo se entrena en 4 subconjuntos y se eval칰a en el restante, repitiendo este proceso 5 veces.

for degree in range(1, 5):  # Probar con grados de 1 a 5
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly = poly.fit_transform(X)
    poly_reg_model = LinearRegression()
    
    # Evaluar el modelo con cross-validation
    scores = cross_val_score(poly_reg_model, X_poly, y, cv=5, scoring='neg_mean_squared_error')
    print(f"Grado {degree} - RMSE: {(-scores.mean())**0.5}")


# ##### Selecci칩n del grado 칩ptimo del polinomio que equilibra el ajuste y evita el sobreajuste.
# 
# 1.- Itera sobre diferentes grados del polinomio (1 a 4).
# 
# 2.- Genera caracter칤sticas polin칩micas para cada grado.
# 
# 3.- Entrena y eval칰a el modelo usando validaci칩n cruzada para medir su desempe침o con el RMSE promedio.
# 
# scoring='neg_mean_squared_error':La m칠trica utilizada es el MSE negativo (para que sea compatible con cross_val_score).
# 
# ##### Sobreajuste (Overfitting):
# Para grados m치s altos (por ejemplo, grado 4 o 5), el modelo puede ajustarse demasiado a los datos de entrenamiento, lo que lleva a un sobreajuste. 
# 
# ##### Subajuste (Underfitting):
# Para grados m치s bajos, el modelo podr칤a ser demasiado simple para capturar la relaci칩n compleja entre las variables, lo que tambi칠n podr칤a aumentar el RMSE promedio.
# 
# 
# ##### La idea es probar diferentes grados del polinomio y ver cu치l proporciona el mejor rendimiento en promedio a lo largo de los pliegues de la validaci칩n cruzada. A pesar de que los valores de RMSE puedan variar entre los pliegues (y entre diferentes grados), lo que realmente nos interesa es saber cu치l grado tiene el menor RMSE promedio a trav칠s de todos los pliegues.

# Visualizamos la diferencia entre los precios actuales y los que el modelo logr칩 predecir
plt.scatter(y_test, poly_y_predict, color='#029386')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Valores calculados')
plt.grid(axis='y', alpha=0.5)
plt.show()


# Se genera un gr치fico de dispersi칩n que compara los valores reales (actuales) de la variable objetivo con las predicciones realizadas por el modelo.
# 
# ##### 1.- Definici칩n de parametros para generar el grafico de dispersi칩n
# 
# y_test:
# Contiene los valores reales (actuales) de la variable objetivo.
# 
# poly_y_predict:
# Contiene las predicciones generadas por el modelo.
# 
# color='#029386': 
# Define el color de los puntos en el gr치fico.
# 
# ##### 2.- Etiquetas de los ejes:
# 
# xlabel: Etiqueta el eje X como "Actual" (valores reales).
# 
# ylabel: Etiqueta el eje Y como "Predicted" (valores predichos).

# ##### L칤nea ideal:
# 
# Si las predicciones fueran perfectas, todos los puntos deber칤an alinearse sobre una l칤nea recta con pendiente 1 (l칤nea Y = X).
# 
# ##### Comportamiento observado:
# La mayor칤a de los puntos est치n cercanos a la l칤nea ideal, lo que indica que el modelo realiza predicciones precisas con la mayoria de los datos. Sin embargo, se observa cierta dispersi칩n en la parte superior derecha, lo que sugiere que el modelo tiene variaciones para los valores reales m치s altos.
# 
# ##### Conclusi칩n:
# 
# El modelo tiene un buen desempe침o con la mayoria de los datos, con predicciones ajustadas a los valores reales, pero existe un margen de error para valores extremos (posiblemente outliers).

plt.figure(figsize=(12,8), dpi=1000)
sns.displot(y_test - poly_y_predict, color='#029386')
plt.title('Histogram of Residuals')
plt.xlabel('Residuals')
plt.ylabel('Frequency')

plt.axvline(-3, color='red', ymax=1)  # L칤neas verticales hasta el tope
plt.axvline(3, color='red', ymax=1)
plt.axhline(59, color='red', xmin=0, xmax=1)  # L칤nea horizontal de borde a borde

plt.show()


# ##### Forma de los residuos:
# 
# La mayor칤a de los residuos est치n centrados cerca de 0, lo cual es deseable, ya que indica que las predicciones no est치n sistem치ticamente sesgadas. Hay una distribuci칩n muy concentrada en torno a valores residuales peque침os.
# 
# ##### L칤neas rojas:
# 
# Las l칤neas verticales en -3 y 3 destacan residuos que podr칤an considerarse outliers o valores at칤picos.
# La l칤nea horizontal en y = 59 marca un umbral de frecuencia m치xima.
# 
# ##### Comportamiento:
# 
# El histograma sugiere que el modelo funciona bien para la mayor칤a de las predicciones, pero hay algunos valores extremos (residuos grandes) en los bordes de la distribuci칩n.

# Guardamos el modelo
joblib.dump(poly_reg_model, 'modelo_entrenado.pkl')


# Guardamos el transformador PolynomialFeatures
joblib.dump(poly, 'transformador_polynomial.pkl')


# Cerrar la conexi칩n
if connection.is_connected():
    connection.close()
    print("Conexi칩n cerrada")


